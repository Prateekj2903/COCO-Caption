{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence as keras_seq\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.applications import inception_resnet_v2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePreprocessor():\n",
    "    IMAGE_SIZE = (299, 299, 3)\n",
    "    \n",
    "    def __init__(self, image_augmentation=None):\n",
    "        \n",
    "        self.active_config = config.Config()\n",
    "        self.image_data_generator = ImageDataGenerator()\n",
    "        self.image_augmentation = image_augmentation or self.active_config.image_augmentation\n",
    "        \n",
    "    def preprocess_images(self, imgs_path):\n",
    "        img_batch = [self.preprocess_single_image(img_path) for img_path in imgs_path]\n",
    "        return np.array(img_batch)\n",
    "        \n",
    "        \n",
    "    def preprocess_single_image(self, img_path):\n",
    "        img = load_img(img_path, target_size=self.IMAGE_SIZE)\n",
    "        img = img_to_array(img)\n",
    "        \n",
    "        if self.image_augmentation:\n",
    "            img = self.image_data_generator.random_transform(img)\n",
    "        img = inception_resnet_v2.preprocess_input(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionPreprocessor():\n",
    "    EOS_TOKEN = 'xeosx'\n",
    "    GLOVE_DIR = \"D:\\\\Datasets\\\\Glove.6B\\\\glove.6B.300d.txt\"\n",
    "    \n",
    "    def __init__(self, clean_descriptions=None, embedding_size=None):\n",
    "        self.active_config = Config()\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.clean_descriptions = clean_descriptions or self.active_config.clean_descriptions\n",
    "        self.use_pre_trained_word_embeddings = self.active_config.use_pre_trained_word_embeddings\n",
    "        self.embedding_size = embedding_size or self.active_config.embedding_size\n",
    "        \n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        word_index = self.tokenizer.word_index\n",
    "        return sorted(word_index, key=word_index.get)\n",
    "    \n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.tokenizer.index_word.keys())\n",
    "        \n",
    "    def fit_captions(self, captions_list):\n",
    "        if self.clean_descriptions:\n",
    "            captions_list = self.description_cleaner(captions_list)\n",
    "        captions_list = self.eos_adder(captions_list)\n",
    "        self.tokenizer.fit_on_texts(captions_list)\n",
    "        self.max_len = self.get_max_len(captions_list)\n",
    "        if self.use_pre_trained_word_embeddings:\n",
    "            self.calculate_word_embeddings()\n",
    "\n",
    "    def get_max_len(self, captions_list):\n",
    "        return max(len(caption_sequence) for caption_sequence in self.tokenizer.texts_to_sequences(captions_list))\n",
    "        \n",
    "    def calculate_word_embeddings(self):\n",
    "        self.embedding_index = self.load_pretrained_word_embeddings()\n",
    "        self.embedding_matrix = np.zeros(size=(self.vocabulary_size+1, self.embedding_size))\n",
    "        \n",
    "        for word, index in self.tokenizer.word_index.items():\n",
    "            embedding_matrix[index-1, :] = embedding_index[word]\n",
    "        \n",
    "        \n",
    "    def load_pretrained_word_embeddings(self):\n",
    "        embeddings_index = {}\n",
    "        f = open(self.GLOVE_DIR, 'rb')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word.decode('utf-8')] = coefs\n",
    "        f.close()\n",
    "        return embeddings_index\n",
    "    \n",
    "    def encode_captions(self, captions, preprocess=False):\n",
    "        captions = self.eos_adder(captions)\n",
    "        encoded_captions = self.tokenizer.texts_to_sequences(captions)\n",
    "        if preprocess:\n",
    "            return self.preprocess_batch(encoded_captions)\n",
    "        else:\n",
    "            return encoded_captions\n",
    "    \n",
    "    def preprocess_batch(self, captions_list):\n",
    "        captions = keras_seq.pad_sequences(captions_list, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        captions_extended1 = keras_seq.pad_sequences(captions, maxlen=self.max_len+1, padding='post')\n",
    "        captions_extended1 = np.expand_dims(captions_extended1, -1)\n",
    "        captions_one_hot = to_categorical(captions_extended1)\n",
    "        \n",
    "        captions_decreased = captions.copy()\n",
    "        captions_decreased[captions_decreased > 0] -= 1\n",
    "        captions_one_hot_shifted = captions_one_hot[:,:,1:]\n",
    "                \n",
    "        captions_input = captions_decreased\n",
    "        captions_output = captions_one_hot_shifted\n",
    "        return captions_input, captions_output\n",
    "        \n",
    "    def description_cleaner(self, captions_list):\n",
    "        # TODO clean descriptions\n",
    "        return captions_list\n",
    "        \n",
    "    def eos_adder(self, captions_list):\n",
    "        return [caption + ' ' + self.EOS_TOKEN for caption in captions_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
